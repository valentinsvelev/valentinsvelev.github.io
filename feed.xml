<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://valentinsvelev.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://valentinsvelev.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-24T20:09:41+00:00</updated><id>https://valentinsvelev.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">My Experience with Publishing an R Package on CRAN</title><link href="https://valentinsvelev.github.io/blog/2026/cran/" rel="alternate" type="text/html" title="My Experience with Publishing an R Package on CRAN"/><published>2026-01-24T00:00:00+00:00</published><updated>2026-01-24T00:00:00+00:00</updated><id>https://valentinsvelev.github.io/blog/2026/cran</id><content type="html" xml:base="https://valentinsvelev.github.io/blog/2026/cran/"><![CDATA[<p>Under construction…</p>]]></content><author><name></name></author><category term="software"/><summary type="html"><![CDATA[How to create an R package? How to publish the R package on The Comprehensive R Archive Network (CRAN)? How to avoid Hadley Wickham as a reviewer? Well, I actually cannot help with the latter, but I can give some insights for the first two questions.]]></summary></entry><entry><title type="html">Evaluating an Agentic AI Application - Summarizing my Experience Integrating DeepEval with Arize Phoenix at Codify</title><link href="https://valentinsvelev.github.io/blog/2025/agentic-ai-eval/" rel="alternate" type="text/html" title="Evaluating an Agentic AI Application - Summarizing my Experience Integrating DeepEval with Arize Phoenix at Codify"/><published>2025-10-03T00:00:00+00:00</published><updated>2025-10-03T00:00:00+00:00</updated><id>https://valentinsvelev.github.io/blog/2025/agentic-ai-eval</id><content type="html" xml:base="https://valentinsvelev.github.io/blog/2025/agentic-ai-eval/"><![CDATA[<p><strong>Note</strong>: This article was published on Codify AG’s website. Check it out <a href="https://www.codify.ch/post/evaluating-ai-agents-with-deepeval-and-arize-phoenix-lessons-from-our-integration-journey">here</a>.</p> <h3 id="what-this-blog-post-is-about">What this Blog Post is About</h3> <p>Today, AI agents are everywhere. One of the biggest challenges in the new era of MLOps isn’t just building an innovative agentic AI tool; it’s proving that it actually works, consistently and reliably. How do you measure “helpfulness”? How do you track down the root cause of (subtle) hallucinations? Standard ML metrics like precision, recall, and accuracy or NLP metrics like BLEU, MAUVE, and BERTScore simply don’t cut it. And what about tracking an agent’s actions and thoughts? How can those be evaluated? At present, these questions constitute some of the most critical considerations in the evaluation of LLM and agent outputs. After experimenting with different frameworks, we decided on <a href="https://deepeval.com/docs/getting-started">DeepEval</a> for our evaluation pipeline and <a href="https://phoenix.arize.com/">Arize Phoenix</a> for collecting and analyzing the traces of our agents.</p> <h3 id="why-deepeval-and-arize-phoenix">Why DeepEval and Arize Phoenix?</h3> <p>DeepEval is an open-source platform designed for evaluating LLMs and agents using what are known as “LLM-as-a-judge” metrics. This technique leverages state-of-the-art LLMs to assess the quality of an AI application’s output. The core idea is that since these powerful models were trained with Reinforcement Learning from Human Feedback (RLHF), their judgment often aligns well with human evaluation (see <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html">Zheng et al., 2023</a>). We decided to use DeepEval over alternative platforms such as RAGAs or Arize Phoenix’s built-in evaluation tools because of its ease-of-use, customizability, and versatility.</p> <p>Arize Phoenix is an open-source platform for collecting and analyzing traces in an AI application. It provides the observability layer we need to understand the inner workings of our agent, from its initial thoughts to its final actions.</p> <h3 id="our-integration-journey-what-we-found">Our Integration Journey: What We Found</h3> <p>While both frameworks work well on their own, making them work in tandem presented several challenges:</p> <ul> <li> <p><strong>Challenges with custom metrics in DeepEval:</strong> While DeepEval is versatile, we encountered some difficulties when fine-tuning our custom evaluation metrics. Creating nuanced, domain-specific metrics that accurately captured the performance of our agent required a significant amount of trial and error. An example is the high sensitivity to semantic and syntactic choices for the judge’s prompt.</p> </li> <li> <p><strong>Difficulties with the integration of DeepEval and Arize Phoenix:</strong> The primary hurdle was integrating DeepEval metrics with Arize Phoenix to run and log our evaluation experiments. This proved difficult because the available documentation did not provide sufficient guidance on configuring Arize Phoenix for custom AI applications. More specifically, one of the biggest challenges is running an experiment with our custom DeepEval metrics. Evaluators are functions that are applied to each data point and specified in the “evaluators” argument in “run_experiment()”. While Arize Phoenix’s documentation provides a simple example for how to define a custom metric, it is not helpful for DeepEval metrics. After scouring the documentation and the source code, we found that the solution was to use the decorator “@create_evaluator”.</p> </li> <li> <p><strong>Running an experiment in Arize Phoenix with “run_experiment()”:</strong> Adapting this method to our required set-up is a key problem to be solved. Some of the most time-consuming challenges are:</p> </li> <li> <p><strong>The “dataset” argument:</strong> Running an evaluation with “run_experiment()” in Arize Phoenix requires uploading a dataset and then downloading it again. This is because you cannot use a local dataset, such as a DataFrame in your environment, to run an evaluation. You need a Dataset object instead, which you obtain by downloading a previously uploaded dataset. This “two-step dance” felt unintuitive, especially for large(r) datasets and multiple concurrent experiment runs.</p> </li> <li> <p><strong>The “task” argument:</strong> If your dataset already includes the LLM or agent responses, you must use a workaround for the task argument by specifying a dummy task that simply returns the response. This is because the “task” argument cannot be empty.</p> </li> <li> <p><strong>Data accessibility issues in Arize Phoenix:</strong> A major workflow challenge we discovered is that you cannot easily access the results of a previously run experiment. To analyze our evaluation results, we had to manually query the underlying database and combine the data ourselves. This was again not easy as there is very little documentation for how the database is structured.</p> </li> <li> <p><strong>A plethora of outdated documentation for Arize Phoenix:</strong> As mentioned previously, one recurring frustration was Arize Phoenix’s documentation. When searching for guidance on Arize Phoenix or its integrations, we often stumbled upon outdated blog posts, GitHub issues, or old versions of docs. This meant we were frequently cross-referencing multiple sources and testing configurations by hand just to figure out what worked with the current release.</p> </li> <li> <p><strong>Parallelization pitfalls in Arize Phoenix:</strong> We also ran into challenges when trying to scale up evaluation runs. Arize Phoenix provides concurrency controls, but in practice we found subtle differences in behavior between synchronous and asynchronous execution. For instance, when running evaluations asynchronously with the “concurrency” argument in “run_experiment()”, we had to set it to 1 because otherwise it would return an error.</p> </li> <li> <p><strong>Running an experiment from our web app:</strong> After setting up the experiment locally, we needed to get it running in Docker. This required setting the PHOENIX_HOST environment variable to enable the connection, which was undocumented and something we only discovered by inspecting the source code. We also encountered persistent timeout issues: even with a high timeout value configured (“timeout” argument in “run_experiment()”), the evaluation would start, complete, and then restart in an infinite loop, repeatedly showing the message “Worker timeout, requeuing.”</p> </li> </ul> <h3 id="final-thoughts">Final Thoughts</h3> <p>Overall, both DeepEval and Arize Phoenix are powerful tools that fill critical gaps in the agent evaluation workflow. DeepEval helps us move beyond traditional metrics by using LLMs as evaluators, while Phoenix gives us the observability we need to diagnose failures and trace agent reasoning. But integrating the two is still an early-stage effort; it requires patience, manual workarounds, and a willingness to dive into undocumented behavior.</p> <p>If you’re considering adopting these tools together, our advice is to budget extra time for debugging, expect to build custom patches, and don’t be afraid to get your hands dirty in the source code.</p>]]></content><author><name></name></author><category term="llm"/><category term="agentic-ai"/><category term="software"/><summary type="html"><![CDATA[How can you make sure that your AI agents' output is truthful and faithful to your query? This is what inspired our journey integrating the evaluation platform DeepEval with the LLM trace platform Arize Phoenix. In this blog post, I will present the challenges I faced in this journey and how I solved them.]]></summary></entry><entry><title type="html">A Brief Introduction to Natural Language Generation (NLG)</title><link href="https://valentinsvelev.github.io/blog/2024/nlg/" rel="alternate" type="text/html" title="A Brief Introduction to Natural Language Generation (NLG)"/><published>2024-09-14T00:00:00+00:00</published><updated>2024-09-14T00:00:00+00:00</updated><id>https://valentinsvelev.github.io/blog/2024/nlg</id><content type="html" xml:base="https://valentinsvelev.github.io/blog/2024/nlg/"><![CDATA[<p>In this blog post, I dive into some of the mathematical foundations of <strong>natural language generation (NLG)</strong>.</p> <p>In language modeling, the likelihood of generating a word \(w_i\) is given by the conditional probability \(P(w_i \mid w_1, \ldots, w_{i-1})\).</p> <p>To generate a coherent and relevant sequence of \(t\) words using <strong>causal language modeling (CLM)</strong>, three components are essential:</p> <ul> <li>a language model \(M\),</li> <li>a prompt \(X\),</li> <li>and a decoding algorithm.</li> </ul> <hr/> <h2 id="prompting">Prompting</h2> <p>Prompts are task-specific instructions used for guiding a model’s output (Amatriain, 2024; Sahoo et al., 2024). In other words, prompts are the user’s input.</p> <p>The model \(M\) generates the subsequent text by maximizing the probability of each word \(w_i\), conditioned not only on the previously generated words \(w_1, \ldots, w_{i-1}\), but also on the prompt<br/> \(X = x_1, \ldots, x_m\):</p> \[P(w_1, \ldots, w_t \mid X) = P(w_1 \mid X) \cdot \prod_{i=2}^t P(w_i \mid X, w_1, \ldots, w_{i-1})\] <p>where \(t\) is the length of the generated text sequence (Bengio et al., 2003; Vaswani et al., 2017).</p> <p>A common way to find effective prompts is <strong>prompt engineering</strong>, i.e., the systematic crafting of prompts. Below I cover some of the most widely used techniques.</p> <hr/> <h3 id="zero-shot-prompting">Zero-Shot Prompting</h3> <p><strong>Zero-shot prompting</strong> (Radford et al., 2019) directly instructs a model to perform a task without examples:</p> <blockquote> <p><em>Classify the text into neutral, negative or positive.</em><br/> <em>Text: I think the vacation is okay.</em><br/> <em>Sentiment:</em></p> </blockquote> <p>Domain-specific fine-tuning or <em>instruction tuning</em> improves zero-shot performance (Wei et al., 2022).</p> <hr/> <h3 id="few-shot-prompting">Few-Shot Prompting</h3> <p><strong>Few-shot prompting</strong> (Brown et al., 2020) provides a small number of examples (“shots”) in the prompt:</p> <blockquote> <p><em>This is awesome! // Negative</em><br/> <em>This is bad! // Positive</em><br/> <em>Wow that movie was rad! // Positive</em><br/> <em>What a horrible show! //</em></p> </blockquote> <p>Few-shot prompting is most effective when the model is sufficiently large (Kaplan et al., 2020).</p> <hr/> <h3 id="chain-of-thought-cot-prompting">Chain-of-Thought (CoT) Prompting</h3> <p><strong>Chain-of-Thought prompting</strong> (Wei et al., 2022) enables multi-step reasoning by providing intermediate reasoning steps:</p> <blockquote> <p><strong>Q:</strong> Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls.<br/> <strong>A:</strong> Roger started with 5 balls. Two cans contain 6 balls.<br/> \(5 + 6 = 11\). The answer is <strong>11</strong>.</p> <p><strong>Q:</strong> The cafeteria had 23 apples. If they used 20 and bought 6 more, how many apples do they have?</p> </blockquote> <p>CoT can be combined with zero-shot or few-shot prompting (Kojima et al., 2022).</p> <hr/> <h3 id="tree-of-thoughts-tot-prompting">Tree-of-Thoughts (ToT) Prompting</h3> <p><strong>Tree-of-Thoughts prompting</strong> (Long, 2023; Yao et al., 2023) encourages self-evaluation by maintaining a tree of reasoning paths:</p> <blockquote> <p><em>Imagine three different experts answering this question.</em><br/> <em>Each writes down one step of their thinking.</em><br/> <em>They share it with the group and continue.</em><br/> <em>If any expert realizes they are wrong, they leave.</em><br/> <em>The question is…</em></p> </blockquote> <hr/> <h2 id="decoding-algorithms">Decoding Algorithms</h2> <p>Decoding algorithms transform probability distributions into coherent natural language. The choice of decoding algorithm significantly affects output quality and diversity.</p> <hr/> <h3 id="greedy-search">Greedy Search</h3> <p><strong>Greedy search</strong> selects the most probable token at each timestep:</p> \[y_t = \arg \max_y P(y \mid y_1, \ldots, y_{t-1}, x)\] <p>Its main limitation is lack of diversity and error accumulation.</p> <hr/> <h3 id="beam-search">Beam Search</h3> <p><strong>Beam search</strong> maintains multiple candidate sequences (“beams”). The scoring function is:</p> \[\mathcal{L}(y_1, \ldots, y_t) = \sum_{k=1}^t \log P(y_k \mid y_1, \ldots, y_{k-1}, x)\] <p>This reduces the risk of missing high-probability sequences but can still lack diversity.</p> <hr/> <h3 id="top-k-sampling">Top-\(k\) Sampling</h3> <p><strong>Top-\(k\) sampling</strong> (Fan et al., 2018) restricts sampling to the \(k\) most likely tokens:</p> \[V^{(k)} = \{ w \in V \mid \text{rank}(P(w)) \le k \}\] <p>The distribution is renormalized:</p> \[P^*(w) = \begin{cases} P(w) / Z &amp; \text{if } w \in V^{(k)} \\ 0 &amp; \text{otherwise} \end{cases}\] <p>Fixed \(k\) can lead to generic or incoherent text (Holtzman et al., 2020).</p> <hr/> <h3 id="nucleus-top-p-sampling">Nucleus (Top-\(p\)) Sampling</h3> <p><strong>Nucleus sampling</strong> selects the smallest vocabulary \(V^{(p)}\) whose cumulative probability exceeds \(p\):</p> \[\sum_{w \in V^{(p)}} P(w) \ge p\] <p>Typically \(0.7 \le p \le 0.95\).</p> <hr/> <h3 id="temperature-sampling">Temperature Sampling</h3> <p>Temperature rescales logits:</p> \[P^*(w_i) = \frac{\exp(z_i / t)}{\sum_j \exp(z_j / t)}\] <p>Lower \(t\) → deterministic, higher \(t\) → more creative.</p> <hr/> <h3 id="eta-sampling">\(\eta\)-Sampling</h3> <p><strong>\(\eta\)-sampling</strong> (Hewitt et al., 2022) uses entropy-based truncation:</p> \[\mathcal{C}(y_{&lt;t}) = \{y \in V \mid P(y \mid y_{&lt;t}) &gt; \eta\}\] <p>with [ \eta = \min(\epsilon, \sqrt{\epsilon} e^{-H(P)}) ]</p> <hr/> <h3 id="locally-typical-sampling">Locally Typical Sampling</h3> <p>Locally typical sampling (Meister et al., 2023) minimizes divergence between conditional entropy and log-probability:</p> \[\min_{\mathcal{C}} \sum_{y \in \mathcal{C}} \left| H(Y_t \mid Y_{&lt;t}) + \log P(y \mid y_{&lt;t}) \right|\] <p>subject to:</p> \[\sum_{y \in \mathcal{C}} P(y \mid y_{&lt;t}) \ge \tau\] <hr/> <h2 id="domain-specific-fine-tuning">Domain-Specific Fine-Tuning</h2> <p>Domain-specific fine-tuning adapts a pretrained model to a dataset \(D\):</p> \[\theta^* = \arg \min_\theta \sum_{(x_i, y_i) \in D} -\log P(y_i \mid x_i, \theta)\] <p>Parameters are updated iteratively:</p> \[\theta \leftarrow \theta - \eta \nabla_\theta L(\theta)\] <p>Regularization yields:</p> \[\theta^* = \arg \min_\theta \left[ \sum_{(x_i, y_i) \in D} -\log P(y_i \mid x_i, \theta) + \lambda R(\theta) \right]\] <hr/> <h2 id="concluding-remarks">Concluding Remarks</h2> <p>Prompting, decoding algorithms, and domain-specific fine-tuning are all powerful mechanisms for controlling the quality, coherence, and diversity of machine-generated text.</p>]]></content><author><name></name></author><category term="math"/><category term="nlg-basics"/><summary type="html"><![CDATA[Natural language generation (NLG), i.e., the generation of text using language models and their causal language modeling capabilities, is not as black box as people make it out to be. In fact, a user can control many parameters. (Note that it is still basically like magic and that me and others are just trying our best)]]></summary></entry><entry><title type="html">A Brief Introduction to (some of) the Math Behind the Transformer</title><link href="https://valentinsvelev.github.io/blog/2024/transformer/" rel="alternate" type="text/html" title="A Brief Introduction to (some of) the Math Behind the Transformer"/><published>2024-09-14T00:00:00+00:00</published><updated>2024-09-14T00:00:00+00:00</updated><id>https://valentinsvelev.github.io/blog/2024/transformer</id><content type="html" xml:base="https://valentinsvelev.github.io/blog/2024/transformer/"><![CDATA[<style>.algorithm-box{border:1px solid var(--global-divider-color);border-radius:8px;padding:1.2rem 1.4rem;margin:1.5rem 0;background-color:var(--global-bg-secondary-color)}.algorithm-box .MathJax{font-size:.95em}.algorithm-box{box-shadow:0 2px 8px rgba(0,0,0,0.06)}</style> <p>The Transformer architecture was first introduced in 2017 by a team of researchers at Google (cf. Vaswani et al., 2017). Originally, their model was developed for machine translation, i.e., the automated translation of text across different languages. However, researchers soon realized that Transformer-based models significantly outperform traditional machine learning models in almost all areas (exceptions include: random forest regression for a classification task using tabular data and logistic regression for the prediction of tsunamis). Today almost all state-of-the-art generative models, such as OpenAI’s GPT-5, Meta AI’s LLaMA 3.3, Google’s Gemini 3, and Anthropic’s Claude 4.5, are based on the Transformer architecture. To get a better understanding of what goes on behind these and other Transformer-based models, I will explain (some) of the math behind the Transformer. This blog post will cover (scaled) dot-product self-attention, (masked) multi-head self-attention, layer normalization, feed forward neural network, linear layer, softmax function, and positional encoding.</p> <div style="display:flex; justify-content:center;"> <img src="/assets/img/transformer.PNG" style="width:438px; height:559.5px;" alt="Transformer architecture"/> </div> <p style="display:flex; justify-content:center;"> <em>Figure 1: The Transformer Architecture (Vaswani et al., 2017)</em> </p> <hr/> <h3 id="dot-product-self-attention">Dot-product self-attention</h3> <p>The dot-product self-attention mechanism is a crucial component that enables a Transformer model the importance of different elements in a sequence relative to each other. The mechanism relies on three key components: queries $Q$, keys $K$, and values $V$, which are derived from the input sequence. Queries represent what each token is “looking for” in other tokens, keys represent the features of the tokens that will be matched with the queries, and values hold the information that will be aggregated and passed on to the next layer based on the attention weights.</p> <p>To obtain these components, each input token undergoes a linear transformation, a process in which the input vector is multiplied by a learned weight matrix. A learned weight matrix is a matrix of parameters that is initialized randomly and then updated during the training process. These parameters are “learned” by the model through backpropagation, where the model adjusts the weights to minimize the error between its predictions and the true outputs. Formally, for each token, the query $Q$, key $K$, and value $V$ are computed as:</p> \[Q=XW^{(q)}\] \[K=XW^{(k)}\] \[V=XW^{(v)}\] <p>where</p> <ul> <li> <p>$X \in \mathbb{R}^{N \times D}$ is the input matrix of positional token embeddings,</p> </li> <li> <p>$W^{(q)} \in \mathbb{R}^{D \times d_k}$, $W^{(k)} \in \mathbb{R}^{D \times d_k}$, and $W^{(v)} \in \mathbb{R}^{D \times d_v}$ are the learned weight matrices that map the input into query, key, and value spaces, respectively, and</p> </li> <li> <p>$Q \in \mathbb{R}^{N \times d_k}$, $K \in \mathbb{R}^{N \times d_k}$, and $V \in \mathbb{R}^{N \times d_v}$ are the resulting matrices of queries, keys, and values, where $d_k$ is the dimensionality of the query and key vectors, and $d_v$ is the dimensionality of the value vectors.</p> </li> </ul> <p>These transformations allow the model to project the input into different vector spaces to perform attention calculations. Note that in some textbooks, for example in Prince (2024), a bias term $\beta$ is included, making the formulas:</p> \[Q = \beta_q1^T+\Omega_qX\] \[K = \beta_k1^T+\Omega_kX\] \[V = \beta_v1^T+\Omega_vX\] <p>where $\beta_q, \beta_k, \beta_v$ are the biases, $1^T$ is a $N \times 1$ vector of ones that ensures the first term can be added to the second term, and $\Omega_q, \Omega_k, \Omega_v$ are the learned weight matrices. Once we have obtained the queries, keys, and values, the (scaled) dot-product self-attention can be computed as follows:</p> <p>The first step in the self-attention process is to compute the <strong>dot-product</strong> between the query and key matrices. Each query vector $Q_i$ is compared to every key vector $K_j$ to measure the similarity between tokens. This results in an attention score matrix $QK^T \in \mathbb{R}^{N \times N}$, where each entry $(i,j)$ in the matrix represents how much attention the $i$-th token should pay to the $j$-th token. This similarity is crucial because it tells the model which tokens in the sequence are most relevant to each other.</p> <p>The second step is <strong>scaling</strong> the attention scores by dividing them by $\sqrt {d_k}$, where $d_k$ is the dimensionality of the key vectors. This is formally expressed as:</p> \[\frac{QK^T}{\sqrt{d_k}}\] <p>This scaling is necessary because as the dimensionality $d_k$ grows, the dot-product values can become large, which can lead to gradients that are too sharp and make training unstable. By scaling the scores, the values remain within a range that makes the softmax function more effective in differentiating the attention between tokens, leading to smoother learning.</p> <p>Next, the <strong>softmax</strong> function is applied to the scaled attention scores to convert them into attention weights $\text{a}_{ij}$. Mathematically, this can be expressed as:</p> \[\text{a}_{ij} = \frac{\exp\left(\frac{(QK^T)_{ij}}{\sqrt{d_k}}\right)}{\sum_{j=1}^{n} \exp\left(\frac{(QK^T)_{ij}}{\sqrt{d_k}}\right)}\] <p>It ensures that the attention weights are normalized—each row of the matrix sums to 1—so that they can be interpreted as probabilities. This tells the model how much focus the $i$-th token should place on each of the other tokens $j$ in the sequence. The softmax function emphasizes the most relevant tokens (those with higher scores) while reducing the influence of less relevant tokens. Finally, the attention weights are used to compute a <strong>weighted sum</strong> of the value vectors. The attention weights are applied to the values $V$, and the sum of these weighted values produces the output for each token:</p> \[\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>This step is what allows the model to aggregate information from the entire sequence, with each token focusing on the most relevant tokens (based on the computed attention weights) to form its representation. This weighted aggregation is key to the model’s ability to capture long-range dependencies and contextual information across the input sequence.</p> <p>Taking everything together, we obtain the following formula for computing scaled dot-product self-attention:</p> \[\text{Y} = \text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>Algorithmically, scaled dot-product self-attention can be expressed as (cf. Bishop &amp; Bishop, 2024, p. 367):</p> <div class="algorithm-box"> $$ \begin{array}{l} \textbf{Algorithm 1: Scaled dot-product self-attention} \\[6pt] \textbf{Input: } \\[4pt] \bullet\ \text{Set of tokens as a matrix} X \in \mathbb{R}^{N \times D} : \{x_1, ..., x_N\} \\[2pt] \bullet\ \text{Weight matrices} \{W^{(q)}, W^{(k)}\} \in \mathbb{R}^{D \times d_k} \text{and} W^{(v)} \in \mathbb{R}^{D \times d_v} \\[4pt] \textbf{Output: } Y \in \mathbb{R}^{N \times d_v} \\[8pt] Q = XW^{(q)} \\ K = XW^{(k)} \\ V = XW^{(v)} \\[6pt] \textbf{return } \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right)V \end{array} $$ </div> <hr/> <h3 id="multi-head-self-attention">Multi-head self-attention</h3> <p>In natural language, some patterns might be relevant to tense whereas others might be associated with vocabulary. Using a single attention head can lead to averaging over these effects and therefore imprecise results. To avoid this, we can use multiple self-attention mechanisms in parallel, each with its own set of learnable parameters that control the computation of the query, key, and value matrices. This is analogous to using multiple different filters in each layer of a convolutional network.</p> <p>For a multi-head self-attention mechanism with $H$ heads indexed by $h = 1, …, H$, the query, key, and value matrices can be calculated using:</p> \[Q_h = XW^{(q)}_h\] \[K_h = XW^{(k)}_h\] \[V_h = XW^{(v)}_h\] <p>or with the bias terms:</p> \[Q_h = \beta_{qh}1^T+\Omega_{qh}X\] \[K_h = \beta_{kh}1^T+\Omega_{kh}X\] \[V_h = \beta_{vh}1^T+\Omega_{vh}X\] <p>The $h$-th head can then be written as:</p> \[H_h = \text{Attention}(Q_h, K_h, V_h)\] <p>Finally, the matrices $H_1, …, H_H$ are concatenated into a single matrix and the result is linearly transformed using a matrix $W^{(o)}$ to obtain the final result:</p> \[Y = \text{Concat}(H_1, ..., H_H)W^{(o)}\] <p>Algorithmically, multi-head self-attention can be written as (cf. Bishop &amp; Bishop, 2024, p. 368):</p> <div class="algorithm-box"> $$ \begin{array}{l} \textbf{Algorithm 2: Multi-head self-attention} \\[8pt] \textbf{Function: } \text{MultiHead}(Q, K, V) \\[6pt] \textbf{Input:} \\[4pt] \bullet\ \text{Set of tokens as a matrix} X \in \mathbb{R}^{N \times D} : \{x_1, \ldots, x_N\} \\[2pt] \bullet\ \text{Query weight matrices} \{W^{(q)}_1, \ldots, W^{(q)}_H\} \in \mathbb{R}^{d_k \times d_k} \\[2pt] \bullet\ \text{Key weight matrices} \{W^{(k)}_1, \ldots, W^{(k)}_H\} \in \mathbb{R}^{d_k \times d_k} \\[2pt] \bullet\ \text{Value weight matrices} \{W^{(v)}_1, \ldots, W^{(v)}_H\} \in \mathbb{R}^{d_k \times d_v} \\[2pt] \bullet\ \text{Output weight matrices} W^{(o)} \in \mathbb{R}^{H d_v \times d_k} \\[8pt] \textbf{Output:} \\[4pt] Y \in \mathbb{R}^{N \times d_k} : \{y_1, \ldots, y_N\} \\[10pt] \textit{// compute self-attention for each head} \\[4pt] \textbf{for } h = 1, \ldots, H \ \textbf{do} \\[4pt] \quad Q_h = X W^{(q)}_h, \quad K_h = X W^{(k)}_h, \quad V_h = X W^{(v)}_h \\[4pt] \quad H_h = \text{Attention}(Q_h, K_h, V_h), \quad H_h \in \mathbb{R}^{N \times d_v} \\[4pt] \textbf{end for} \\[8pt] H = \text{Concat}(H_1, \ldots, H_H) \\[4pt] \textbf{return } Y = H W^{(o)} \end{array} $$ </div> <hr/> <h3 id="cross-attention">Cross-attention</h3> <p>Cross-attention refers to a variation of the self-attention mechanism, where the output of the encoder is used in the decoder. Specifically, the query matrix $Q$ comes from the decoder, while the key matrix $K$ and the value matrix $V$ come from the encoder. Mathematically, this can be expressed as:</p> \[\text{Attention}(Q, K^{'}, V^{'}) = \text{softmax}\left(\frac{QK^{'T}}{\sqrt{d_k}}\right)V^{'}\] <p>where the superscript $’$ refers to matrices from the encoder. This mechanism allows the decoder to leverage the contextualized representations learned by the encoder, facilitating more informed and accurate generation of outputs, such as translations or summaries.</p> <hr/> <h3 id="residual-connections">Residual connections</h3> <p>Residual connections or skip connections, introduced by <a href="https://ieeexplore.ieee.org/document/7780459">He et al. (2016)</a>, are shortcuts that bypass a given sublayer and add the input of a sublayer to its output, which helps prevent the loss of information in deeper layers and ensures that the model can continue to learn effectively even as the depth increases. They applied around each multi-head attention sublayer and feed-forward network. Mathematically, residual connections can be expressed as:</p> \[Z = Y + X\] <p>where $Y$ is the output of a given sublayer (e.g., of the multi-head self-attention layer or the feed-forward neural network) and $X$ is the original input matrix.</p> <hr/> <h3 id="layer-normalization">Layer normalization</h3> <p>Layer normalization, introduced by <a href="https://arxiv.org/abs/1607.06450">Ba et al. (2016)</a>, is a technique for normalizing each data sample across the the feature dimension $D$. It improves training efficiency by fixing the mean and the variance of the summed inputs within each layer, which reduces the “covariate shift problem”. For a given input vector $X = {x_1, …, x_D}$, where $D$ is the number of hidden units or features, layer normalization computes the empirical mean $\mu$ and variance $\sigma^2$ for each input:</p> \[\mu_n = \frac{1}{D} \sum^M_{i=1}x_{ni}, \quad \sigma^2_n = \frac{1}{D} \sum^M_{i=1}(x_{ni} - \mu_n)^2\] <p>The input is then transformed as follows:</p> \[z_{ni} = \frac{x_{ni} - \mu_n}{\sqrt{\sigma^2_n + \epsilon}}\] <p>This standardization is analogous to the z-standardization in statistics. Finally, the normalized variable is scaled by $\gamma$ and shifted by $\delta$:</p> \[y_{ni} = \gamma z_{ni} + \delta\] <p>The “Add &amp; Norm” sublayer can then be written as:</p> \[Z = \text{LayerNorm}(Y + X)\] <hr/> <h3 id="feed-forward-neural-network">Feed forward neural network</h3> <p>The position-wise feed-forward neural network (FFNN) in the Transformer consists of two linear layers with a non-linear activation function, ReLU, between them. The FFNN can be written as:</p> \[\text{FFNN}(X) = \text{max}(0, XW_1 + b_1)W_2 + b_2\] <p>where:</p> <ul> <li> <p>$X \in \mathbb{R}^{N \times D}$ is the input matrix,</p> </li> <li> <p>$W_1 \in \mathbb{R}^{D \times d_{ff}}$ and $W_2 \in \mathbb{R}^{d_{ff} \times D}$ are the learned weight matrices ($d_{ff}$ is an intermediate dimensionality that is typically larger than $D$, cf. Vaswani et al., 2017) ,</p> </li> <li> <p>$b_1 \in \mathbb{R}^{d_{ff}}$ and $b_2 \in \mathbb{R}^D$ are the bias vectors, and</p> </li> <li> <p>$\text{max}(0,\cdot)$ is the ReLU activation function.</p> </li> </ul> <p>The FFNN is applied to the output of the “Add &amp; Norm” sublayer:</p> \[\tilde{X} = \text{LayerNorm}(\text{MLP}(Z) + Z)\] <p>Algorithmically, the transformer layer (multi-head self-attention, add &amp; norm, FFNN) can be expressed as (cf. Bishop &amp; Bishop, 2024, p. 367):</p> <div class="algorithm-box"> $$ \begin{array}{l} \textbf{Algorithm 3: Transformer layer} \\[8pt] \textbf{Function: } \text{TransformLayer}(Y) \\[6pt] \textbf{Input:} \\[4pt] \bullet\ X \in \mathbb{R}^{N \times D} : \{x_1, \ldots, x_N\} \\[2pt] \bullet\ \text{Multi-head self-attention layer parameters} \\[2pt] \bullet\ \text{Feed-forward neural network parameters} \\[8pt] \textbf{Output:} \\[4pt] \tilde{X} \in \mathbb{R}^{N \times D} : \{\tilde{x}_1, \ldots, \tilde{x}_N\} \\[10pt] Z = \text{LayerNorm}(Y + X) \\[4pt] \tilde{X} = \text{LayerNorm}(\text{MLP}(Z) + Z) \\[8pt] \textbf{return } \tilde{X} \end{array} $$ </div> <hr/> <h3 id="linear-layer">Linear layer</h3> <p>A linear layer or fully connected layer applies a learned linear transformation to the input. It changes the dimensionality of the output from the previous layer to project the model’s hidden representations to a vocabulary space $V$, thereby preparing the hidden states for the softmax layer. The linear transformation can be defined as:</p> \[\tilde{Y} = \tilde{X}W + b\] <p>where $\tilde{X} \in \mathbb{R}^{N \times D}$ is the output from the previous sublayer, $W \in \mathbb{R}^{D \times V}$ is the learned weight matrix that defines how the input is transformed, and $b \in \mathbb{R}^V$ is the bias vector.</p> <hr/> <h3 id="softmax-layer">Softmax layer</h3> <p>As a final step in the transformer architecture, a softmax activation function is applied to the output of the linear layer to convert the raw scores into probabilities. The softmax function is defined as:</p> \[P(y_i = v \mid \tilde{Y}_i) = \frac{\exp(\tilde{Y}_{i,v})}{\sum_{v' \in V}\exp(\tilde{Y}_{i, v'})}\] <p>where $P(y_i = v \mid \tilde{Y}<em>i)$ is the probability that the model predicts the $i$-th token to be word $v$ from the vocabulary $V$, and $\tilde{Y}</em>{i,v}$ is the output score for word $v$ from the linear layer at the $i$-th position in the sequence. The softmax function exponentiates these scores and normalizes them by dividing by the sum of the exponentiated scores over the entire vocabulary, ensuring that the resulting probabilities sum to 1. This allows the model to output a probability distribution across all possible vocabulary tokens, with higher scores corresponding to higher probabilities of the predicted word being correct.</p> <hr/> <h3 id="positional-encoding">Positional encoding</h3> <p>In the transformer architecture, the matrices $W^{(q)}_h$, $W^{(k)}_h$, and $W^{(v)}_h$ are shared across all input tokens, as is the subsequent neural network. As a result, the transformer exhibits the property that permuting the order of the input tokens, i.e., rearranging the rows of the input matrix $X$, leads to the same permutation in the rows of the output matrix $\tilde{X}$ (see Algorithm 3). This means that a transformer is equivariant with respect to input permutations, which allows for efficient, massively parallel processing and enables the model to learn long-range dependencies as effectively as short-range ones. However, this independence from token order poses a challenge when dealing with sequential data like natural language, where the meaning of a sentence can drastically change with different word orders. For example, the sentences “The food was bad, not good at all” and “The food was good, not bad at all” contain the same tokens but convey very different meanings. Thus, to overcome this limitation and account for the importance of word order in sequential tasks, the transformer architecture incorporates <strong>positional encoding</strong> to inject token order information into the model. Positional encodings allow the transformer to differentiate between tokens based on their position in the sequence, ensuring that the model can capture both the content and the relative order of words. We will therefore construct a position encoding vector $r_n$ associated with each input position $n$ and then combine this vector with the associated input token embedding $x_n$ by adding them:</p> \[\tilde{x}_n = x_n + r_n\] <p>Note that this requires that $r_n$ and $x_n$ have the same dimensionality. Bishop &amp; Bishop (2024, p. 372) have stated that “[a]n ideal positional encoding should provide a unique representation for each position, it should be bounded, it should generalize to longer sequences, and it should have a consistent way to express the number of steps between any two input vectors irrespective of their absolute position because the relative position of tokens is often more important than the absolute position”.</p> <p>However, many approaches to positional encoding exist (<a href="https://direct.mit.edu/coli/article/48/3/733/111478/Position-Information-in-Transformers-An-Overview">Dufter et al., 2022</a>). Here I will describe a technique based on sinusoidal functions used in Vaswani et al. (2017). For a given position $n$ the components of the positional-encoding vector $r_{ni}$ are given by:</p> \[r_{ni} = \left\{ \begin{array}{ll} \text{sin}(\frac{n}{T^{i/D}}) &amp; \text{if } i \text{ is even}, \\ \text{cos}(\frac{n}{T^{(i-1)/D}}) &amp; \text{if } i \text{ is odd}, \end{array} \right.\] <p>where $i$ is the position of a token and $T$ is $10^4$ (cf. Vaswani et al., 2017).</p> <hr/> <h3 id="concluding-remarks">Concluding remarks</h3> <p>In this blog post, I outlined the math behind the Transformer and its individual components. It should be noted that this blog post only provides a high-level overview and that most (if not all) of today’s Transformer-based models solely use the decoder (the right side in Figure 1). Although state-of-the-art NLG models are based on Vaswani et al. (2017)’s Transformer, most of them have different variants of the components presented in this blog post (pre-normalization, SwiGLU activation function). If you want more information, you can check out Bishop (2006), Deisenroth et al. (2020), Fleuret (2023), Prince (2024), and Bishop &amp; Bishop (2024). Additionally, the blog post by Jay Alammar found <a href="https://jalammar.github.io/illustrated-transformer/">here</a> is great for understanding the linear algebra used in Transformers.</p>]]></content><author><name></name></author><category term="math"/><category term="nlg-basics"/><summary type="html"><![CDATA[The Transformer architecture, introduced in 2017 by Google, is the basis for every state-of-the-art generative AI model today. Understanding (some of) the math behind it is essential for understanding the Transformer itself and today's generative AI models.]]></summary></entry></feed>