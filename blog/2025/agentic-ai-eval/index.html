<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Evaluating an Agentic AI Application: Summarizing my Experience Integrating DeepEval with Arize Phoenix at Codify | Valentin S. Velev </title> <meta name="author" content="Valentin S. Velev"> <meta name="description" content="How can you make sure that your AI agents' output is truthful and faithful to your query? This is what inspired our journey integrating the evaluation platform DeepEval with the LLM trace platform Arize Phoenix. In this blog post, I will present the challenges I faced in this journey and how I solved them."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://valentinsvelev.github.io/blog/2025/agentic-ai-eval/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Valentin</span> S. Velev </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Evaluating an Agentic AI Application: Summarizing my Experience Integrating DeepEval with Arize Phoenix at Codify</h1> <p class="post-meta"> Created on October 03, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/agentic-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> agentic-ai</a>   <a href="/blog/tag/software"> <i class="fa-solid fa-hashtag fa-sm"></i> software</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>Note</strong>: This article was published on Codify AG’s website. Check it out <a href="https://www.codify.ch/post/evaluating-ai-agents-with-deepeval-and-arize-phoenix-lessons-from-our-integration-journey" rel="external nofollow noopener" target="_blank">here</a>.</p> <h3 id="what-this-blog-post-is-about">What this Blog Post is About</h3> <p>Today, AI agents are everywhere. One of the biggest challenges in the new era of MLOps isn’t just building an innovative agentic AI tool; it’s proving that it actually works, consistently and reliably. How do you measure “helpfulness”? How do you track down the root cause of (subtle) hallucinations? Standard ML metrics like precision, recall, and accuracy or NLP metrics like BLEU, MAUVE, and BERTScore simply don’t cut it. And what about tracking an agent’s actions and thoughts? How can those be evaluated? At present, these questions constitute some of the most critical considerations in the evaluation of LLM and agent outputs. After experimenting with different frameworks, we decided on <a href="https://deepeval.com/docs/getting-started" rel="external nofollow noopener" target="_blank">DeepEval</a> for our evaluation pipeline and <a href="https://phoenix.arize.com/" rel="external nofollow noopener" target="_blank">Arize Phoenix</a> for collecting and analyzing the traces of our agents.</p> <h3 id="why-deepeval-and-arize-phoenix">Why DeepEval and Arize Phoenix?</h3> <p>DeepEval is an open-source platform designed for evaluating LLMs and agents using what are known as “LLM-as-a-judge” metrics. This technique leverages state-of-the-art LLMs to assess the quality of an AI application’s output. The core idea is that since these powerful models were trained with Reinforcement Learning from Human Feedback (RLHF), their judgment often aligns well with human evaluation (see <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html" rel="external nofollow noopener" target="_blank">Zheng et al., 2023</a>). We decided to use DeepEval over alternative platforms such as RAGAs or Arize Phoenix’s built-in evaluation tools because of its ease-of-use, customizability, and versatility.</p> <p>Arize Phoenix is an open-source platform for collecting and analyzing traces in an AI application. It provides the observability layer we need to understand the inner workings of our agent, from its initial thoughts to its final actions.</p> <h3 id="our-integration-journey-what-we-found">Our Integration Journey: What We Found</h3> <p>While both frameworks work well on their own, making them work in tandem presented several challenges:</p> <ul> <li> <p><strong>Challenges with custom metrics in DeepEval:</strong> While DeepEval is versatile, we encountered some difficulties when fine-tuning our custom evaluation metrics. Creating nuanced, domain-specific metrics that accurately captured the performance of our agent required a significant amount of trial and error. An example is the high sensitivity to semantic and syntactic choices for the judge’s prompt.</p> </li> <li> <p><strong>Difficulties with the integration of DeepEval and Arize Phoenix:</strong> The primary hurdle was integrating DeepEval metrics with Arize Phoenix to run and log our evaluation experiments. This proved difficult because the available documentation did not provide sufficient guidance on configuring Arize Phoenix for custom AI applications. More specifically, one of the biggest challenges is running an experiment with our custom DeepEval metrics. Evaluators are functions that are applied to each data point and specified in the “evaluators” argument in “run_experiment()”. While Arize Phoenix’s documentation provides a simple example for how to define a custom metric, it is not helpful for DeepEval metrics. After scouring the documentation and the source code, we found that the solution was to use the decorator “@create_evaluator”.</p> </li> <li> <p><strong>Running an experiment in Arize Phoenix with “run_experiment()”:</strong> Adapting this method to our required set-up is a key problem to be solved. Some of the most time-consuming challenges are:</p> </li> <li> <p><strong>The “dataset” argument:</strong> Running an evaluation with “run_experiment()” in Arize Phoenix requires uploading a dataset and then downloading it again. This is because you cannot use a local dataset, such as a DataFrame in your environment, to run an evaluation. You need a Dataset object instead, which you obtain by downloading a previously uploaded dataset. This “two-step dance” felt unintuitive, especially for large(r) datasets and multiple concurrent experiment runs.</p> </li> <li> <p><strong>The “task” argument:</strong> If your dataset already includes the LLM or agent responses, you must use a workaround for the task argument by specifying a dummy task that simply returns the response. This is because the “task” argument cannot be empty.</p> </li> <li> <p><strong>Data accessibility issues in Arize Phoenix:</strong> A major workflow challenge we discovered is that you cannot easily access the results of a previously run experiment. To analyze our evaluation results, we had to manually query the underlying database and combine the data ourselves. This was again not easy as there is very little documentation for how the database is structured.</p> </li> <li> <p><strong>A plethora of outdated documentation for Arize Phoenix:</strong> As mentioned previously, one recurring frustration was Arize Phoenix’s documentation. When searching for guidance on Arize Phoenix or its integrations, we often stumbled upon outdated blog posts, GitHub issues, or old versions of docs. This meant we were frequently cross-referencing multiple sources and testing configurations by hand just to figure out what worked with the current release.</p> </li> <li> <p><strong>Parallelization pitfalls in Arize Phoenix:</strong> We also ran into challenges when trying to scale up evaluation runs. Arize Phoenix provides concurrency controls, but in practice we found subtle differences in behavior between synchronous and asynchronous execution. For instance, when running evaluations asynchronously with the “concurrency” argument in “run_experiment()”, we had to set it to 1 because otherwise it would return an error.</p> </li> <li> <p><strong>Running an experiment from our web app:</strong> After setting up the experiment locally, we needed to get it running in Docker. This required setting the PHOENIX_HOST environment variable to enable the connection, which was undocumented and something we only discovered by inspecting the source code. We also encountered persistent timeout issues: even with a high timeout value configured (“timeout” argument in “run_experiment()”), the evaluation would start, complete, and then restart in an infinite loop, repeatedly showing the message “Worker timeout, requeuing.”</p> </li> </ul> <h3 id="final-thoughts">Final Thoughts</h3> <p>Overall, both DeepEval and Arize Phoenix are powerful tools that fill critical gaps in the agent evaluation workflow. DeepEval helps us move beyond traditional metrics by using LLMs as evaluators, while Phoenix gives us the observability we need to diagnose failures and trace agent reasoning. But integrating the two is still an early-stage effort; it requires patience, manual workarounds, and a willingness to dive into undocumented behavior.</p> <p>If you’re considering adopting these tools together, our advice is to budget extra time for debugging, expect to build custom patches, and don’t be afraid to get your hands dirty in the source code.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Valentin S. Velev. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: January 24, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>