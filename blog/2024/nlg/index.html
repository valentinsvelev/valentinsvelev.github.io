<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A Brief Introduction to Natural Language Generation (NLG) | Valentin S. Velev </title> <meta name="author" content="Valentin S. Velev"> <meta name="description" content="Natural language generation (NLG), i.e., the generation of text using language models and their causal language modeling capabilities, is not as black box as people make it out to be. In fact, a user can control many parameters. (Note that it is still basically like magic and that me and others are just trying our best)"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://valentinsvelev.github.io/blog/2024/nlg/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Valentin</span> S. Velev </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">A Brief Introduction to Natural Language Generation (NLG)</h1> <p class="post-meta"> Created on September 14, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>   <a href="/blog/tag/nlg-basics"> <i class="fa-solid fa-hashtag fa-sm"></i> nlg-basics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In this blog post, I dive into some of the mathematical foundations of <strong>natural language generation (NLG)</strong>.</p> <p>In language modeling, the likelihood of generating a word \(w_i\) is given by the conditional probability \(P(w_i \mid w_1, \ldots, w_{i-1})\).</p> <p>To generate a coherent and relevant sequence of \(t\) words using <strong>causal language modeling (CLM)</strong>, three components are essential:</p> <ul> <li>a language model \(M\),</li> <li>a prompt \(X\),</li> <li>and a decoding algorithm.</li> </ul> <hr> <h2 id="prompting">Prompting</h2> <p>Prompts are task-specific instructions used for guiding a model’s output (Amatriain, 2024; Sahoo et al., 2024). In other words, prompts are the user’s input.</p> <p>The model \(M\) generates the subsequent text by maximizing the probability of each word \(w_i\), conditioned not only on the previously generated words \(w_1, \ldots, w_{i-1}\), but also on the prompt<br> \(X = x_1, \ldots, x_m\):</p> \[P(w_1, \ldots, w_t \mid X) = P(w_1 \mid X) \cdot \prod_{i=2}^t P(w_i \mid X, w_1, \ldots, w_{i-1})\] <p>where \(t\) is the length of the generated text sequence (Bengio et al., 2003; Vaswani et al., 2017).</p> <p>A common way to find effective prompts is <strong>prompt engineering</strong>, i.e., the systematic crafting of prompts. Below I cover some of the most widely used techniques.</p> <hr> <h3 id="zero-shot-prompting">Zero-Shot Prompting</h3> <p><strong>Zero-shot prompting</strong> (Radford et al., 2019) directly instructs a model to perform a task without examples:</p> <blockquote> <p><em>Classify the text into neutral, negative or positive.</em><br> <em>Text: I think the vacation is okay.</em><br> <em>Sentiment:</em></p> </blockquote> <p>Domain-specific fine-tuning or <em>instruction tuning</em> improves zero-shot performance (Wei et al., 2022).</p> <hr> <h3 id="few-shot-prompting">Few-Shot Prompting</h3> <p><strong>Few-shot prompting</strong> (Brown et al., 2020) provides a small number of examples (“shots”) in the prompt:</p> <blockquote> <p><em>This is awesome! // Negative</em><br> <em>This is bad! // Positive</em><br> <em>Wow that movie was rad! // Positive</em><br> <em>What a horrible show! //</em></p> </blockquote> <p>Few-shot prompting is most effective when the model is sufficiently large (Kaplan et al., 2020).</p> <hr> <h3 id="chain-of-thought-cot-prompting">Chain-of-Thought (CoT) Prompting</h3> <p><strong>Chain-of-Thought prompting</strong> (Wei et al., 2022) enables multi-step reasoning by providing intermediate reasoning steps:</p> <blockquote> <p><strong>Q:</strong> Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls.<br> <strong>A:</strong> Roger started with 5 balls. Two cans contain 6 balls.<br> \(5 + 6 = 11\). The answer is <strong>11</strong>.</p> <p><strong>Q:</strong> The cafeteria had 23 apples. If they used 20 and bought 6 more, how many apples do they have?</p> </blockquote> <p>CoT can be combined with zero-shot or few-shot prompting (Kojima et al., 2022).</p> <hr> <h3 id="tree-of-thoughts-tot-prompting">Tree-of-Thoughts (ToT) Prompting</h3> <p><strong>Tree-of-Thoughts prompting</strong> (Long, 2023; Yao et al., 2023) encourages self-evaluation by maintaining a tree of reasoning paths:</p> <blockquote> <p><em>Imagine three different experts answering this question.</em><br> <em>Each writes down one step of their thinking.</em><br> <em>They share it with the group and continue.</em><br> <em>If any expert realizes they are wrong, they leave.</em><br> <em>The question is…</em></p> </blockquote> <hr> <h2 id="decoding-algorithms">Decoding Algorithms</h2> <p>Decoding algorithms transform probability distributions into coherent natural language. The choice of decoding algorithm significantly affects output quality and diversity.</p> <hr> <h3 id="greedy-search">Greedy Search</h3> <p><strong>Greedy search</strong> selects the most probable token at each timestep:</p> \[y_t = \arg \max_y P(y \mid y_1, \ldots, y_{t-1}, x)\] <p>Its main limitation is lack of diversity and error accumulation.</p> <hr> <h3 id="beam-search">Beam Search</h3> <p><strong>Beam search</strong> maintains multiple candidate sequences (“beams”). The scoring function is:</p> \[\mathcal{L}(y_1, \ldots, y_t) = \sum_{k=1}^t \log P(y_k \mid y_1, \ldots, y_{k-1}, x)\] <p>This reduces the risk of missing high-probability sequences but can still lack diversity.</p> <hr> <h3 id="top-k-sampling">Top-\(k\) Sampling</h3> <p><strong>Top-\(k\) sampling</strong> (Fan et al., 2018) restricts sampling to the \(k\) most likely tokens:</p> \[V^{(k)} = \{ w \in V \mid \text{rank}(P(w)) \le k \}\] <p>The distribution is renormalized:</p> \[P^*(w) = \begin{cases} P(w) / Z &amp; \text{if } w \in V^{(k)} \\ 0 &amp; \text{otherwise} \end{cases}\] <p>Fixed \(k\) can lead to generic or incoherent text (Holtzman et al., 2020).</p> <hr> <h3 id="nucleus-top-p-sampling">Nucleus (Top-\(p\)) Sampling</h3> <p><strong>Nucleus sampling</strong> selects the smallest vocabulary \(V^{(p)}\) whose cumulative probability exceeds \(p\):</p> \[\sum_{w \in V^{(p)}} P(w) \ge p\] <p>Typically \(0.7 \le p \le 0.95\).</p> <hr> <h3 id="temperature-sampling">Temperature Sampling</h3> <p>Temperature rescales logits:</p> \[P^*(w_i) = \frac{\exp(z_i / t)}{\sum_j \exp(z_j / t)}\] <p>Lower \(t\) → deterministic, higher \(t\) → more creative.</p> <hr> <h3 id="eta-sampling">\(\eta\)-Sampling</h3> <p><strong>\(\eta\)-sampling</strong> (Hewitt et al., 2022) uses entropy-based truncation:</p> \[\mathcal{C}(y_{&lt;t}) = \{y \in V \mid P(y \mid y_{&lt;t}) &gt; \eta\}\] <p>with [ \eta = \min(\epsilon, \sqrt{\epsilon} e^{-H(P)}) ]</p> <hr> <h3 id="locally-typical-sampling">Locally Typical Sampling</h3> <p>Locally typical sampling (Meister et al., 2023) minimizes divergence between conditional entropy and log-probability:</p> \[\min_{\mathcal{C}} \sum_{y \in \mathcal{C}} \left| H(Y_t \mid Y_{&lt;t}) + \log P(y \mid y_{&lt;t}) \right|\] <p>subject to:</p> \[\sum_{y \in \mathcal{C}} P(y \mid y_{&lt;t}) \ge \tau\] <hr> <h2 id="domain-specific-fine-tuning">Domain-Specific Fine-Tuning</h2> <p>Domain-specific fine-tuning adapts a pretrained model to a dataset \(D\):</p> \[\theta^* = \arg \min_\theta \sum_{(x_i, y_i) \in D} -\log P(y_i \mid x_i, \theta)\] <p>Parameters are updated iteratively:</p> \[\theta \leftarrow \theta - \eta \nabla_\theta L(\theta)\] <p>Regularization yields:</p> \[\theta^* = \arg \min_\theta \left[ \sum_{(x_i, y_i) \in D} -\log P(y_i \mid x_i, \theta) + \lambda R(\theta) \right]\] <hr> <h2 id="concluding-remarks">Concluding Remarks</h2> <p>Prompting, decoding algorithms, and domain-specific fine-tuning are all powerful mechanisms for controlling the quality, coherence, and diversity of machine-generated text.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Valentin S. Velev. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: January 26, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>