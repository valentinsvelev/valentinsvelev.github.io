
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer (Blog) &ndash; Valentin Velev</title>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap">
    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
<body>
    <header>
        <div class="header-container">
            <div class="header-info">
                <h1>Valentin Velev</h1>
                <p>MSc Data Science Student | Researcher | Aspiring Data Scientist</p>
                <p>
                    <i class="fas fa-envelope"></i>
                    <a href="mailto:valentin.velev@uni-konstanz.de" style="color:white">valentin.velev@uni-konstanz.de</a>
                </p>
            </div>
            <nav class="header-nav">
                <ul>
                    <li><a href="../index.html">About Me</a></li>
                    <li><a href="../research.html">Research</a></li>
                    <li><a href="../projects.html">Projects</a></li>
                    <li><a href="../blog.html">Blog</a></li>
                    <li><a href="../cv.html">CV</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <div class="title-container">
        <h2 class="title-text">A Brief Introduction to (some of) the Math behind the Transformer</h2>
    </div>

    <div class="wrapper">
        <div class="container">
            <p>The Transformer archtitecture was first introduced in 2017 by a team of researchers at Google (cf. Vaswani et al. 2017). Originally, their model was for machine translation, i.e., the automated translation of text across different languages. However, researchers soon realized that Transformer-based models significantly outperform traditional machine learning models in almost all areas (exceptions include: 1. random forest regression for a classification task using tabular data and 2. logit regression for the prediction of tsunamis). Today almost all state-of-the-art generative models, such as OpenAI's GPT-4o, Meta AI's LLaMA 3.1, Google's Gemini 1.5, and Anthropic's Claude 3.5, are based on the Transformer architecture. To get a better understanding of what goes on behind these and other Transformer-based models, I will explain (some) of the math behind the Transformer, including ...</p>
            <p>...</p>
        </div>
    </div>

    <footer>
        <div class="footer">
            <p>Last update: 2024-09-02. Hosted on GitHub Pages.</p>
            <p>&copy; 2024 Valentin Velev. All Rights Reserved.</p>
        </div>
    </footer>

    <script src="../scripts.js"></script>
</body>
</html>
